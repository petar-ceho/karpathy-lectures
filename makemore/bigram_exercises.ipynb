{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset \n",
    "#load  dataset to get list of strings  \n",
    "words=open('names.txt','r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of words in a dataset 32033\n",
      "shortest word in the dataset 2\n",
      "longest word in the dataset 15\n"
     ]
    }
   ],
   "source": [
    "#some info of the dataset\n",
    "print('total number of words in a dataset',len(words))\n",
    "print('shortest word in the dataset',min(len(w)for w in words))\n",
    "print('longest word in the dataset',max(len(w)for w in words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary\n",
    "chars=sorted(list(set(''.join(words))))\n",
    "#string to integer\n",
    "stoi={s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.']=0\n",
    "#integer to string \n",
    "itos={i:s for s,i in stoi.items()}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see.\n",
    "\n",
    "train: 2.2833642959594727\n",
    "\n",
    "dev loss 2.277104616165161\n",
    "\n",
    "test loss 2.277104616165161\n",
    "\n",
    "so the model performs almost the same on all 3 dataset's \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(words):\n",
    "    xs,ys=[],[]\n",
    "    for w in words:\n",
    "        chs=['.']+list(w)+['.']\n",
    "        for ch1,ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n",
    "            ix1=stoi[ch1]\n",
    "            ix2=stoi[ch2]\n",
    "            ix3=stoi[ch3]\n",
    "            xs.append([ix1,ix2])\n",
    "            ys.append(ix3)\n",
    "\n",
    "    xs=torch.tensor(xs)\n",
    "    ys=torch.tensor(ys)\n",
    "    return xs,ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(words)\n",
    "\n",
    "#split 80% train ,10% dev,10% test \n",
    "train_size=int(0.8*len(words))\n",
    "dev_size=int(0.9*len(words))\n",
    "\n",
    "#train,dev,test datasets \n",
    "x_train,x_test=split(words[:train_size])\n",
    "d_train,d_test=split(words[:dev_size])\n",
    "t_train,t_test=split(words[dev_size:])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?  \n",
    "\n",
    "so after 100 epochs we did get a little improvement \n",
    "\n",
    "bigram loss after 100 epochs: 2.4884581565856934\n",
    "\n",
    "trigram loss after 100 epochs: 2.2833642959594727\n",
    "\n",
    "\n",
    "i reduce the learning_rate to be 25,even that its a huge learning rate,probably adding a l1 reqularization would help,and for sure having a activation func(RELU). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4312031269073486\n",
      "3.0970945358276367\n",
      "2.885296106338501\n",
      "2.7657711505889893\n",
      "2.68744158744812\n",
      "2.630394220352173\n",
      "2.586986780166626\n",
      "2.5529510974884033\n",
      "2.5256099700927734\n",
      "2.503190517425537\n",
      "2.4844772815704346\n",
      "2.468609571456909\n",
      "2.454967498779297\n",
      "2.4430928230285645\n",
      "2.432645320892334\n",
      "2.423366069793701\n",
      "2.4150564670562744\n",
      "2.4075615406036377\n",
      "2.400758981704712\n",
      "2.3945512771606445\n",
      "2.3888590335845947\n",
      "2.3836166858673096\n",
      "2.378770351409912\n",
      "2.3742752075195312\n",
      "2.3700928688049316\n",
      "2.3661904335021973\n",
      "2.3625400066375732\n",
      "2.359116315841675\n",
      "2.3558998107910156\n",
      "2.352870464324951\n",
      "2.3500120639801025\n",
      "2.3473100662231445\n",
      "2.344752311706543\n",
      "2.3423261642456055\n",
      "2.340022325515747\n",
      "2.3378305435180664\n",
      "2.3357431888580322\n",
      "2.3337526321411133\n",
      "2.3318519592285156\n",
      "2.3300347328186035\n",
      "2.328295946121216\n",
      "2.326629877090454\n",
      "2.3250324726104736\n",
      "2.3234987258911133\n",
      "2.3220250606536865\n",
      "2.320608139038086\n",
      "2.319244384765625\n",
      "2.3179306983947754\n",
      "2.316664695739746\n",
      "2.3154430389404297\n",
      "2.3142640590667725\n",
      "2.313124895095825\n",
      "2.312023878097534\n",
      "2.3109593391418457\n",
      "2.3099288940429688\n",
      "2.3089308738708496\n",
      "2.3079640865325928\n",
      "2.3070268630981445\n",
      "2.3061182498931885\n",
      "2.3052358627319336\n",
      "2.30437970161438\n",
      "2.303548574447632\n",
      "2.3027405738830566\n",
      "2.301954984664917\n",
      "2.3011913299560547\n",
      "2.300447940826416\n",
      "2.299724817276001\n",
      "2.299020290374756\n",
      "2.298335075378418\n",
      "2.297666549682617\n",
      "2.2970151901245117\n",
      "2.2963802814483643\n",
      "2.2957606315612793\n",
      "2.295156717300415\n",
      "2.2945666313171387\n",
      "2.2939910888671875\n",
      "2.293428897857666\n",
      "2.292879819869995\n",
      "2.2923433780670166\n",
      "2.291818857192993\n",
      "2.291306257247925\n",
      "2.290804862976074\n",
      "2.2903144359588623\n",
      "2.289834976196289\n",
      "2.289365530014038\n",
      "2.2889063358306885\n",
      "2.288456439971924\n",
      "2.288015842437744\n",
      "2.2875845432281494\n",
      "2.2871618270874023\n",
      "2.286747694015503\n",
      "2.286341667175293\n",
      "2.2859437465667725\n",
      "2.2855539321899414\n",
      "2.2851712703704834\n",
      "2.2847957611083984\n",
      "2.2844278812408447\n",
      "2.284066677093506\n",
      "2.283712387084961\n",
      "2.2833642959594727\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "#since we now using 2 char to predict the 3rd one we need to have [27*2,27]  \n",
    "W_trigram=torch.rand((27*2,27),generator=g,requires_grad=True)\n",
    "\n",
    "for k in range(100):\n",
    "    #turn two char into one hot encoded vectors shape [N,2,27] cast to float \n",
    "    xenc=F.one_hot(x_train,num_classes=27).float()\n",
    "    #view returns a new tensor with the same data diff shape [N,27*2] \n",
    "    logits=xenc.view(-1,27*2)@W_trigram\n",
    "    #softmax \n",
    "    counts=logits.exp()\n",
    "    probs=counts/counts.sum(1,keepdims=True)\n",
    "    #negative log likelihood + l2 regularization\n",
    "    loss= -probs[torch.arange(x_test.shape[0]), x_test].log().mean() + 0.01 * (W_trigram**2).mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    #backward pass \n",
    "    W_trigram.grad=None\n",
    "    loss.backward()\n",
    "\n",
    "    #gradient descent\n",
    "    W_trigram.data+=-25*W_trigram.grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate func for  dev,test datasets  \n",
    "def evaluate(features,labels):\n",
    "    xenc=F.one_hot(features,num_classes=27).float()\n",
    "    #we can think that this is like the first layer \n",
    "    logits = xenc.view(-1, 27*2) @ W_trigram \n",
    "    #softmax\n",
    "    counts=logits.exp()\n",
    "    probs=counts/counts.sum(1,keepdims=True)\n",
    "    #negative log likelihood + l2 regularization\n",
    "    loss= -probs[torch.arange(features.shape[0]), labels].log().mean() + 0.001 * (W_trigram**2).mean()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev loss 2.277104616165161\n",
      "test loss 2.277104616165161\n"
     ]
    }
   ],
   "source": [
    "dev_loss=evaluate(d_train,d_test)\n",
    "test_loss=evaluate(d_train,d_test)\n",
    "print('dev loss',dev_loss)\n",
    "print('test loss',test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
