{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load  dataset to get list of strings  \n",
    "words=open('names.txt','r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of words in a dataset 32033\n",
      "shortest word in the dataset 2\n",
      "longest word in the dataset 15\n"
     ]
    }
   ],
   "source": [
    "#little info about the data \n",
    "print('total number of words in a dataset',len(words))\n",
    "print('shortest word in the dataset',min(len(w)for w in words))\n",
    "print('longest word in the dataset',max(len(w)for w in words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary\n",
    "chars=sorted(list(set(''.join(words))))\n",
    "#string to integer\n",
    "stoi={s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.']=0\n",
    "#integer to string \n",
    "itos={i:s for s,i in stoi.items()}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see.\n",
    "\n",
    "    train: 2.2833642959594727\n",
    "\n",
    "    dev loss 2.277104616165161\n",
    "\n",
    "    test loss 2.277104616165161\n",
    "\n",
    "so the model performs almost the same on all 3 dataset's \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(words):\n",
    "    xs,ys=[],[]\n",
    "    for w in words:\n",
    "        chs=['.']+list(w)+['.']\n",
    "        for ch1,ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n",
    "            ix1=stoi[ch1]\n",
    "            ix2=stoi[ch2]\n",
    "            ix3=stoi[ch3]\n",
    "            xs.append([ix1,ix2])\n",
    "            ys.append(ix3)\n",
    "\n",
    "    xs=torch.tensor(xs)\n",
    "    ys=torch.tensor(ys)\n",
    "    return xs,ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(words)\n",
    "\n",
    "#split 80% train ,10% dev,10% test \n",
    "train_size=int(0.8*len(words))\n",
    "dev_size=int(0.9*len(words))\n",
    "\n",
    "#train,dev,test datasets \n",
    "x_train,x_test=split(words[:train_size])\n",
    "d_train,d_test=split(words[:dev_size])\n",
    "t_train,t_test=split(words[dev_size:])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?  \n",
    "\n",
    "so after 100 epochs we did get a little improvement \n",
    "\n",
    "    -bigram loss after 100 epochs: 2.4884581565856934\n",
    "\n",
    "    -trigram loss after 100 epochs: 2.2833642959594727\n",
    "\n",
    "\n",
    "i reduce the learning_rate to be 25,even that its a huge learning rate,probably adding a l1 reqularization would help,and for sure having a activation func(RELU). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4314181804656982\n",
      "2.937809467315674\n",
      "2.7350199222564697\n",
      "2.6303911209106445\n",
      "2.564183235168457\n",
      "2.5189216136932373\n",
      "2.4860715866088867\n",
      "2.4611103534698486\n",
      "2.4414167404174805\n",
      "2.4253995418548584\n",
      "2.4120519161224365\n",
      "2.4007134437561035\n",
      "2.390932559967041\n",
      "2.3823912143707275\n",
      "2.37485671043396\n",
      "2.3681538105010986\n",
      "2.362147569656372\n",
      "2.356731653213501\n",
      "2.351821184158325\n",
      "2.3473458290100098\n",
      "2.343250036239624\n",
      "2.339484930038452\n",
      "2.3360114097595215\n",
      "2.3327958583831787\n",
      "2.3298094272613525\n",
      "2.3270280361175537\n",
      "2.3244307041168213\n",
      "2.3219985961914062\n",
      "2.319716453552246\n",
      "2.3175699710845947\n",
      "2.315547466278076\n",
      "2.3136379718780518\n",
      "2.3118324279785156\n",
      "2.310121774673462\n",
      "2.3084986209869385\n",
      "2.3069570064544678\n",
      "2.305490255355835\n",
      "2.3040926456451416\n",
      "2.302760124206543\n",
      "2.3014872074127197\n",
      "2.3002710342407227\n",
      "2.299107313156128\n",
      "2.29799222946167\n",
      "2.2969233989715576\n",
      "2.295897960662842\n",
      "2.294912576675415\n",
      "2.2939655780792236\n",
      "2.2930550575256348\n",
      "2.292178153991699\n",
      "2.2913334369659424\n",
      "2.2905187606811523\n",
      "2.2897331714630127\n",
      "2.2889745235443115\n",
      "2.2882418632507324\n",
      "2.28753399848938\n",
      "2.286848783493042\n",
      "2.2861862182617188\n",
      "2.2855443954467773\n",
      "2.2849228382110596\n",
      "2.284320592880249\n",
      "2.28373646736145\n",
      "2.2831695079803467\n",
      "2.2826194763183594\n",
      "2.282085418701172\n",
      "2.281567096710205\n",
      "2.2810628414154053\n",
      "2.2805724143981934\n",
      "2.2800958156585693\n",
      "2.2796318531036377\n",
      "2.2791807651519775\n",
      "2.2787411212921143\n",
      "2.278313159942627\n",
      "2.2778961658477783\n",
      "2.277489423751831\n",
      "2.2770931720733643\n",
      "2.2767066955566406\n",
      "2.276329517364502\n",
      "2.2759618759155273\n",
      "2.2756025791168213\n",
      "2.275252103805542\n",
      "2.274909734725952\n",
      "2.2745752334594727\n",
      "2.2742486000061035\n",
      "2.2739288806915283\n",
      "2.2736167907714844\n",
      "2.2733116149902344\n",
      "2.273013114929199\n",
      "2.2727208137512207\n",
      "2.272435426712036\n",
      "2.272155523300171\n",
      "2.2718820571899414\n",
      "2.2716140747070312\n",
      "2.2713513374328613\n",
      "2.27109432220459\n",
      "2.2708425521850586\n",
      "2.2705955505371094\n",
      "2.2703542709350586\n",
      "2.2701170444488525\n",
      "2.2698845863342285\n",
      "2.2696568965911865\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "#since we now using 2 char to predict the 3rd one we need to have [27*2,27]  \n",
    "W_trigram=torch.rand((27*2,27),generator=g,requires_grad=True)\n",
    "l2_rate=0.01\n",
    "learning_rate=40\n",
    "\n",
    "for k in range(100):\n",
    "    #turn two char into one hot encoded vectors shape [N,2,27] cast to float \n",
    "    xenc=F.one_hot(x_train,num_classes=27).float()\n",
    "    #view returns a new tensor with the same data diff shape [N,27*2] \n",
    "    logits=xenc.view(-1,27*2)@W_trigram\n",
    "    #softmax \n",
    "    counts=logits.exp()\n",
    "    probs=counts/counts.sum(1,keepdims=True)\n",
    "    #negative log likelihood + l2 regularization\n",
    "    loss= -probs[torch.arange(x_test.shape[0]), x_test].log().mean() + l2_rate * (W_trigram**2).mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    #backward pass \n",
    "    W_trigram.grad=None\n",
    "    loss.backward()\n",
    "\n",
    "    #gradient descent\n",
    "    W_trigram.data+=-learning_rate*W_trigram.grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate func for  dev,test datasets  \n",
    "def evaluate(features,labels):\n",
    "    xenc=F.one_hot(features,num_classes=27).float()\n",
    "    #we can think that this is like the first layer \n",
    "    logits = xenc.view(-1, 27*2) @ W_trigram \n",
    "    #softmax\n",
    "    counts=logits.exp()\n",
    "    probs=counts/counts.sum(1,keepdims=True)\n",
    "    #negative log likelihood + l2 regularization\n",
    "    loss= -probs[torch.arange(features.shape[0]), labels].log().mean() +  l2_rate * (W_trigram**2).mean()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev loss 2.2701611518859863\n",
      "test loss 2.2701611518859863\n"
     ]
    }
   ],
   "source": [
    "dev_loss=evaluate(d_train,d_test)\n",
    "test_loss=evaluate(d_train,d_test)\n",
    "print('dev loss',dev_loss)\n",
    "print('test loss',test_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n",
    "\n",
    "trying to set the l2_learning_rate>0.01 ex:0.1 is actually giving  worse results.\n",
    "\n",
    "the learning rate when we set it >50 is way to high and anything below 40 i think its a small learning rate(lol) so probably adding here a learning rate decay would help to be able to decrease the learning_rate over epochs.\n",
    "\n",
    "loss archived given in EO1 desc.\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?\n",
    "\n",
    "So the reason is mainly efficiency since computing the derivatives of Softmax and Categorical cross entropy separate is wasteful it can be simplified a lot to get more efficient function to compute the gradients of both functions,and less code :)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4314181804656982\n",
      "2.7966766357421875\n",
      "2.6260831356048584\n",
      "2.540867805480957\n",
      "2.495635986328125\n",
      "2.461956262588501\n",
      "2.442763566970825\n",
      "2.421114921569824\n",
      "2.409801483154297\n",
      "2.3941047191619873\n",
      "2.3860316276550293\n",
      "2.3745193481445312\n",
      "2.3681862354278564\n",
      "2.359614372253418\n",
      "2.354408025741577\n",
      "2.3478848934173584\n",
      "2.3434760570526123\n",
      "2.3384199142456055\n",
      "2.334667444229126\n",
      "2.3306963443756104\n",
      "2.327516555786133\n",
      "2.3243472576141357\n",
      "2.321659803390503\n",
      "2.3190925121307373\n",
      "2.3168270587921143\n",
      "2.3147048950195312\n",
      "2.3127834796905518\n",
      "2.3109941482543945\n",
      "2.3093395233154297\n",
      "2.307792901992798\n",
      "2.3063464164733887\n",
      "2.304987907409668\n",
      "2.30370831489563\n",
      "2.302500009536743\n",
      "2.3013558387756348\n",
      "2.300269603729248\n",
      "2.2992360591888428\n",
      "2.298252582550049\n",
      "2.297316312789917\n",
      "2.296422004699707\n",
      "2.2955665588378906\n",
      "2.2947490215301514\n",
      "2.2939679622650146\n",
      "2.293219566345215\n",
      "2.2925028800964355\n",
      "2.2918152809143066\n",
      "2.2911555767059326\n",
      "2.2905213832855225\n",
      "2.289910316467285\n",
      "2.2893216609954834\n",
      "2.288754463195801\n",
      "2.2882070541381836\n",
      "2.2876780033111572\n",
      "2.287167549133301\n",
      "2.2866742610931396\n",
      "2.2861971855163574\n",
      "2.2857353687286377\n",
      "2.2852890491485596\n",
      "2.284856081008911\n",
      "2.2844371795654297\n",
      "2.284031391143799\n",
      "2.283637523651123\n",
      "2.2832555770874023\n",
      "2.2828848361968994\n",
      "2.2825255393981934\n",
      "2.2821762561798096\n",
      "2.281836748123169\n",
      "2.281506061553955\n",
      "2.281184196472168\n",
      "2.2808706760406494\n",
      "2.2805659770965576\n",
      "2.280268907546997\n",
      "2.279979944229126\n",
      "2.279698610305786\n",
      "2.2794241905212402\n",
      "2.2791566848754883\n",
      "2.2788960933685303\n",
      "2.278641939163208\n",
      "2.2783944606781006\n",
      "2.2781527042388916\n",
      "2.277916669845581\n",
      "2.2776854038238525\n",
      "2.2774603366851807\n",
      "2.2772395610809326\n",
      "2.277024269104004\n",
      "2.276813268661499\n",
      "2.276606559753418\n",
      "2.27640438079834\n",
      "2.276207208633423\n",
      "2.2760138511657715\n",
      "2.275824785232544\n",
      "2.275639533996582\n",
      "2.2754578590393066\n",
      "2.275280237197876\n",
      "2.2751059532165527\n",
      "2.274935483932495\n",
      "2.274768352508545\n",
      "2.274604558944702\n",
      "2.2744438648223877\n",
      "2.2742865085601807\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "#since we now using 2 char to predict the 3rd one we need to have [27*2,27]  \n",
    "W_trigram=torch.rand((27*2,27),generator=g,requires_grad=True)\n",
    "l2_rate=0.01\n",
    "l1_rate=0.01\n",
    "learning_rate=60\n",
    "decay=0.01\n",
    "for k in range(100):\n",
    "    #turn two char into one hot encoded vectors shape [N,2,27] cast to float \n",
    "    xenc=F.one_hot(x_train,num_classes=27).float()\n",
    "    #view returns a new tensor with the same data diff shape [N,27*2] \n",
    "    logits=xenc.view(-1,27*2)@W_trigram\n",
    "    \n",
    "    #relu nonlinearity\n",
    "    relu=nn.ReLU()\n",
    "    relu_out=relu(logits)\n",
    "\n",
    "    #with categorical cross entropy\n",
    "    loss = F.cross_entropy(relu_out, x_test)\n",
    "    loss+l1_rate*(W_trigram.sum())\n",
    "    loss+=l2_rate*(W_trigram**2).mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    #backward pass \n",
    "    W_trigram.grad=None\n",
    "    loss.backward()\n",
    "\n",
    "    #add learning rate decay:\n",
    "    current_learning_rate=learning_rate*(1/(1+decay*k))\n",
    "\n",
    "    #gradient descent\n",
    "    W_trigram.data+=-current_learning_rate*W_trigram.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E06: meta-exercise! Think of a fun/interesting exercise and complete it.\t\n",
    "\n",
    "\t-adding learning rate decay \n",
    "\t\n",
    "\t-adding a relu non-linearity   \n",
    "\n",
    "\t-adding l1 regularization \n",
    "\t\n",
    "code above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o\n",
      "ot\n",
      "otq\n",
      "otqa\n",
      "x\n",
      "xx\n",
      "xxa\n",
      "xxan\n",
      "xxann\n",
      "xxannl\n",
      "xxannli\n",
      "xxannlix\n",
      "xxannlixp\n",
      "xxannlixpw\n",
      "xxannlixpwt\n",
      "xxannlixpwtz\n",
      "xxannlixpwtzk\n",
      "xxannlixpwtzki\n",
      "xxannlixpwtzkis\n",
      "xxannlixpwtzkisa\n",
      "xxannlixpwtzkisal\n",
      "xxannlixpwtzkisalt\n",
      "xxannlixpwtzkisalti\n",
      "xxannlixpwtzkisaltii\n",
      "xxannlixpwtzkisaltiio\n",
      "h\n",
      "ht\n",
      "hta\n",
      "htae\n",
      "a\n",
      "an\n",
      "ani\n",
      "anii\n",
      "aniiw\n",
      "aniiwm\n",
      "aniiwmr\n",
      "aniiwmrz\n",
      "aniiwmrzc\n",
      "aniiwmrzcs\n",
      "aniiwmrzcsm\n",
      "aniiwmrzcsmi\n",
      "r\n",
      "rb\n",
      "rbe\n",
      "rber\n",
      "rberh\n",
      "rberhn\n",
      "rberhna\n",
      "rberhnah\n",
      "rberhnahc\n",
      "rberhnahce\n",
      "rberhnahcee\n",
      "rberhnahceec\n",
      "rberhnahceect\n",
      "rberhnahceecti\n",
      "rberhnahceectia\n",
      "rberhnahceectiah\n",
      "rberhnahceectiahe\n",
      "rberhnahceectiaheh\n",
      "rberhnahceectiahehq\n",
      "rberhnahceectiahehqq\n",
      "rberhnahceectiahehqqn\n",
      "rberhnahceectiahehqqne\n",
      "rberhnahceectiahehqqnet\n",
      "rberhnahceectiahehqqnetl\n",
      "rberhnahceectiahehqqnetlg\n",
      "rberhnahceectiahehqqnetlgo\n",
      "rberhnahceectiahehqqnetlgoc\n",
      "rberhnahceectiahehqqnetlgoce\n",
      "rberhnahceectiahehqqnetlgocee\n",
      "rberhnahceectiahehqqnetlgoceey\n",
      "rberhnahceectiahehqqnetlgoceeyt\n",
      "rberhnahceectiahehqqnetlgoceeyti\n",
      "rberhnahceectiahehqqnetlgoceeytir\n"
     ]
    }
   ],
   "source": [
    "#check whats wrong here in sampling \n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "    out=[]\n",
    "    ix=0\n",
    "    while True:\n",
    "        #linear layer\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27*2).float()\n",
    "        #matrix multiplication\n",
    "        logits=xenc@W_trigram\n",
    "        #relu\n",
    "        relu=nn.ReLU()\n",
    "        relu_out=relu(logits)\n",
    "        #softmax \n",
    "        softmax=nn.Softmax(dim=1)\n",
    "        p=softmax(relu_out)\n",
    "\n",
    "        #sample from multinomial distribution \n",
    "        ix=torch.multinomial(p,num_samples=1,replacement=True,generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix==0:\n",
    "            break\n",
    "        print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
